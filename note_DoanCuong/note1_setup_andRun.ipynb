{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ch·∫°y xong c√°i n√†y r·ªìi th√¨ sao, \n",
    "\n",
    "1. C√≥ bug g√¨ kh√¥ng ??\n",
    "2. Gi·ªù l√†m sao ƒë·ªÉ xem ??\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gi·∫£i th√≠ch script t·ª´ng ph·∫ßn**\n",
    "\n",
    "D∆∞·ªõi ƒë√¢y l√† gi·∫£i th√≠ch t·ª´ng ph·∫ßn c·ªßa script tr√™n, gi√∫p b·∫°n hi·ªÉu r√µ logic v√† vai tr√≤ c·ªßa t·ª´ng b∆∞·ªõc:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. C√†i ƒë·∫∑t c√°c d·ªãch v·ª• c·∫ßn thi·∫øt**\n",
    "| **Th√†nh ph·∫ßn**       | **Vai tr√≤**                                                                                       | **Gi·∫£i th√≠ch**                                                                                           |\n",
    "|----------------------|---------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| `check_command`      | H√†m ki·ªÉm tra n·∫øu l·ªánh ho·∫∑c ph·∫ßn m·ªÅm ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.                                              | In ra th√¥ng b√°o n·∫øu ph·∫ßn m·ªÅm ƒë√£ ƒë∆∞·ª£c c√†i, v√≠ d·ª•: `Java is installed`.                                     |\n",
    "| `verify_installation`| H√†m x√°c nh·∫≠n th∆∞ m·ª•c c·ªßa c√°c c√¥ng c·ª• ƒë√£ ƒë∆∞·ª£c t·∫£i v·ªÅ ƒë√∫ng v·ªã tr√≠.                                   | Ki·ªÉm tra c√°c c√¥ng c·ª• nh∆∞ Kafka, Hadoop c√≥ t·ªìn t·∫°i trong th∆∞ m·ª•c `$HOME` hay kh√¥ng.                        |\n",
    "| `cleanup_old_files`  | X√≥a c√°c file c≈© tr∆∞·ªõc khi t·∫£i v·ªÅ b·∫£n m·ªõi.                                                         | ƒê·∫£m b·∫£o kh√¥ng c√≥ file c≈© b·ªã tr√πng ho·∫∑c g√¢y l·ªói trong qu√° tr√¨nh c√†i ƒë·∫∑t.                                   |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. C√†i ƒë·∫∑t c√°c c√¥ng c·ª• ch√≠nh**\n",
    "#### **(a) Kafka**\n",
    "- **T·∫£i v√† c√†i Kafka**:\n",
    "  ```bash\n",
    "  wget -c \"https://archive.apache.org/dist/kafka/3.6.0/kafka_2.13-3.6.0.tgz\"\n",
    "  ```\n",
    "- T√°c d·ª•ng: C√†i Kafka, c√¥ng c·ª• x·ª≠ l√Ω lu·ªìng d·ªØ li·ªáu th·ªùi gian th·ª±c.\n",
    "- **Ki·ªÉm tra k·∫øt qu·∫£**:\n",
    "  ```bash\n",
    "  $HOME/kafka/bin/kafka-topics.sh --version\n",
    "  ```\n",
    "\n",
    "#### **(b) Hadoop**\n",
    "- **T·∫£i v√† c√†i Hadoop**:\n",
    "  ```bash\n",
    "  wget -c \"https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\"\n",
    "  ```\n",
    "- T√°c d·ª•ng: L∆∞u tr·ªØ v√† x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn ph√¢n t√°n.\n",
    "- **Ki·ªÉm tra k·∫øt qu·∫£**:\n",
    "  ```bash\n",
    "  $HOME/hadoop/bin/hadoop version\n",
    "  ```\n",
    "\n",
    "#### **(c) HBase**\n",
    "- **T·∫£i v√† c√†i HBase**:\n",
    "  ```bash\n",
    "  wget -c \"https://archive.apache.org/dist/hbase/2.5.5/hbase-2.5.5-bin.tar.gz\"\n",
    "  ```\n",
    "- T√°c d·ª•ng: L∆∞u tr·ªØ d·ªØ li·ªáu l·ªõn theo th·ªùi gian th·ª±c.\n",
    "- **Ki·ªÉm tra k·∫øt qu·∫£**:\n",
    "  ```bash\n",
    "  $HOME/hbase/bin/hbase version\n",
    "  ```\n",
    "\n",
    "#### **(d) Spark**\n",
    "- **T·∫£i v√† c√†i Spark**:\n",
    "  ```bash\n",
    "  wget -c \"https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\"\n",
    "  ```\n",
    "- T√°c d·ª•ng: X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn v·ªõi t·ªëc ƒë·ªô cao.\n",
    "- **Ki·ªÉm tra k·∫øt qu·∫£**:\n",
    "  ```bash\n",
    "  $HOME/spark/bin/spark-submit --version\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng**\n",
    "| **Bi·∫øn**          | **Vai tr√≤**                                                               | **Gi·∫£i th√≠ch**                                                                                          |\n",
    "|--------------------|---------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n",
    "| `JAVA_HOME`        | ƒê∆∞·ªùng d·∫´n t·ªõi Java.                                                      | D√πng cho t·∫•t c·∫£ c√°c c√¥ng c·ª• Big Data nh∆∞ Kafka, Hadoop, HBase, Spark.                                   |\n",
    "| `KAFKA_HOME`       | ƒê∆∞·ªùng d·∫´n Kafka.                                                         | D·ªÖ d√†ng ch·∫°y c√°c l·ªánh Kafka.                                                                            |\n",
    "| `HADOOP_HOME`      | ƒê∆∞·ªùng d·∫´n Hadoop.                                                        | S·ª≠ d·ª•ng l·ªánh Hadoop nh∆∞ `hdfs`, `start-dfs.sh`.                                                         |\n",
    "| `HBASE_HOME`       | ƒê∆∞·ªùng d·∫´n HBase.                                                         | Ch·∫°y HBase t·ª´ b·∫•t k·ª≥ th∆∞ m·ª•c n√†o.                                                                       |\n",
    "| `SPARK_HOME`       | ƒê∆∞·ªùng d·∫´n Spark.                                                         | D√πng ƒë·ªÉ ch·∫°y c√°c l·ªánh Spark nh∆∞ `spark-submit`, `spark-shell`.                                          |\n",
    "| `PATH`             | Th√™m t·∫•t c·∫£ c√°c c√¥ng c·ª• v√†o bi·∫øn m√¥i tr∆∞·ªùng h·ªá th·ªëng.                    | ƒê·∫£m b·∫£o c√°c c√¥ng c·ª• c√≥ th·ªÉ ƒë∆∞·ª£c ch·∫°y t·ª´ b·∫•t k·ª≥ n∆°i n√†o trong terminal.                                  |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. C√†i ƒë·∫∑t Python packages**\n",
    "| **Th∆∞ vi·ªán**       | **Vai tr√≤**                                                                                           | **Gi·∫£i th√≠ch**                                                                                           |\n",
    "|--------------------|------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| `requests`         | G·ª≠i y√™u c·∫ßu HTTP, c·∫ßn thi·∫øt cho Docker Compose.                                                      | D√πng ƒë·ªÉ giao ti·∫øp v·ªõi Docker Compose ho·∫∑c t·∫£i d·ªØ li·ªáu t·ª´ API.                                            |\n",
    "| `docker-compose`   | Qu·∫£n l√Ω v√† kh·ªüi ch·∫°y container Docker.                                                               | Gi√∫p ch·∫°y c√°c th√†nh ph·∫ßn nh∆∞ Apache Airflow d·ªÖ d√†ng.                                                     |\n",
    "| `kafka-python`     | Th∆∞ vi·ªán Python ƒë·ªÉ giao ti·∫øp v·ªõi Kafka.                                                              | H·ªó tr·ª£ vi·∫øt producer v√† consumer trong Python.                                                           |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Ki·ªÉm tra v√† x√°c minh**\n",
    "| **B∆∞·ªõc**                           | **C√°ch th·ª±c hi·ªán**                                                         | **K·∫øt qu·∫£ mong ƒë·ª£i**                                                                                     |\n",
    "|------------------------------------|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| Ki·ªÉm tra Java                      | ```java -version```                                                       | Hi·ªÉn th·ªã phi√™n b·∫£n Java.                                                                                 |\n",
    "| Ki·ªÉm tra Kafka                     | ```$HOME/kafka/bin/kafka-topics.sh --version```                           | Hi·ªÉn th·ªã phi√™n b·∫£n Kafka.                                                                                |\n",
    "| Ki·ªÉm tra Hadoop                    | ```$HOME/hadoop/bin/hadoop version```                                     | Hi·ªÉn th·ªã phi√™n b·∫£n Hadoop.                                                                               |\n",
    "| Ki·ªÉm tra HBase                     | ```$HOME/hbase/bin/hbase version```                                       | Hi·ªÉn th·ªã phi√™n b·∫£n HBase.                                                                                |\n",
    "| Ki·ªÉm tra Spark                     | ```$HOME/spark/bin/spark-submit --version```                              | Hi·ªÉn th·ªã phi√™n b·∫£n Spark.                                                                                |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. K·∫øt lu·∫≠n**\n",
    "Script n√†y:\n",
    "- **T·ª± ƒë·ªông h√≥a c√†i ƒë·∫∑t c√°c c√¥ng c·ª• Big Data**: Kafka, Hadoop, HBase, Spark.\n",
    "- **Ki·ªÉm tra v√† x√°c minh**: ƒê·∫£m b·∫£o c√°c c√¥ng c·ª• ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë√∫ng c√°ch.\n",
    "- **Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng**: H·ªó tr·ª£ ch·∫°y c√°c l·ªánh t·ª´ b·∫•t k·ª≥ ƒë√¢u trong terminal.\n",
    "\n",
    "B·∫°n ch·ªâ c·∫ßn:\n",
    "1. Ch·∫°y script: \n",
    "   ```bash\n",
    "   sudo ./commands.sh\n",
    "   ```\n",
    "2. T·∫£i l·∫°i bi·∫øn m√¥i tr∆∞·ªùng:\n",
    "   ```bash\n",
    "   source ~/.bashrc\n",
    "   ```\n",
    "3. X√°c minh t·∫•t c·∫£ c√¥ng c·ª• ƒë√£ ho·∫°t ƒë·ªông. üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "root@DESKTOP-2IQJ5FH:~/GIT/Big-Data-Project_2/Main# ./install_services.sh\n",
    "Installing required services...\n",
    "Installing basic requirements...\n",
    "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease                                    \n",
    "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease                          \n",
    "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                   \n",
    "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
    "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/lib/cnf-update-db\", line 3, in <module>\n",
    "    import apt_pkg\n",
    "ModuleNotFoundError: No module named 'apt_pkg'\n",
    "Reading package lists... Done\n",
    "E: Problem executing scripts APT::Update::Post-Invoke-Success 'if /usr/bin/test -w /var/lib/command-not-found/ -a -e /usr/lib/cnf-update-db; then /usr/lib/cnf-update-db > /dev/null; fi'\n",
    "E: Sub-process returned an error code\n",
    "Reading package lists... Done\n",
    "Building dependency tree... Done\n",
    "Reading state information... Done\n",
    "default-jdk is already the newest version (2:1.11-72build2).\n",
    "lsof is already the newest version (4.93.2+dfsg-1.1build2).\n",
    "net-tools is already the newest version (1.60+git20181103.0eebece-1ubuntu5).\n",
    "gzip is already the newest version (1.10-4ubuntu4.1).\n",
    "python3-apt is already the newest version (2.4.0ubuntu4).\n",
    "tar is already the newest version (1.34+dfsg-1ubuntu0.1.22.04.2).\n",
    "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
    "0 upgraded, 0 newly installed, 0 to remove and 58 not upgraded.\n",
    "Installing Python packages...\n",
    "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
    "Requirement already satisfied: docker-compose in /usr/lib/python3/dist-packages (1.29.2)\n",
    "Requirement already satisfied: kafka-python in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests) (1.26.5)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (3.3)\n",
    "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.0)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2020.6.20)\n",
    "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
    "Cleaning up old files...\n",
    "Installing Kafka...\n",
    "Downloading kafka.tgz (attempt 1/3)...\n",
    "--2024-12-14 23:14:37--  https://archive.apache.org/dist/kafka/3.6.0/kafka_2.13-3.6.0.tgz\n",
    "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
    "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 113257079 (108M) [application/x-gzip]\n",
    "Saving to: ‚Äòkafka.tgz‚Äô\n",
    "\n",
    "kafka.tgz              100%[===========================>] 108.01M   915KB/s    in 77s     \n",
    "\n",
    "2024-12-14 23:15:55 (1.41 MB/s) - ‚Äòkafka.tgz‚Äô saved [113257079/113257079]\n",
    "\n",
    "‚úÖ File integrity check passed for kafka.tgz\n",
    "‚úÖ Kafka installation successful\n",
    "Installing Hadoop...\n",
    "Downloading hadoop.tar.gz (attempt 1/3)...\n",
    "--2024-12-14 23:15:56--  https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
    "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
    "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 730107476 (696M) [application/x-gzip]\n",
    "Saving to: ‚Äòhadoop.tar.gz‚Äô\n",
    "\n",
    "hadoop.tar.gz          100%[===========================>] 696.28M  15.3MB/s    in 65s     \n",
    "\n",
    "2024-12-14 23:17:01 (10.7 MB/s) - ‚Äòhadoop.tar.gz‚Äô saved [730107476/730107476]\n",
    "\n",
    "‚úÖ File integrity check passed for hadoop.tar.gz\n",
    "‚úÖ Hadoop installation successful\n",
    "Installing HBase...\n",
    "Downloading hbase.tar.gz (attempt 1/3)...\n",
    "--2024-12-14 23:17:17--  https://archive.apache.org/dist/hbase/2.5.5/hbase-2.5.5-bin.tar.gz\n",
    "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
    "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 313010492 (299M) [application/x-gzip]\n",
    "Saving to: ‚Äòhbase.tar.gz‚Äô\n",
    "\n",
    "hbase.tar.gz           100%[===========================>] 298.51M  14.0MB/s    in 23s     \n",
    "\n",
    "2024-12-14 23:17:41 (13.0 MB/s) - ‚Äòhbase.tar.gz‚Äô saved [313010492/313010492]\n",
    "\n",
    "‚úÖ File integrity check passed for hbase.tar.gz\n",
    "‚úÖ HBase installation successful\n",
    "Installing Spark...\n",
    "Downloading spark.tgz (attempt 1/3)...\n",
    "--2024-12-14 23:17:45--  https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
    "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 400395283 (382M) [application/x-gzip]\n",
    "Saving to: ‚Äòspark.tgz‚Äô\n",
    "\n",
    "spark.tgz              100%[===========================>] 381.85M  5.42MB/s    in 46s     \n",
    "\n",
    "2024-12-14 23:18:32 (8.34 MB/s) - ‚Äòspark.tgz‚Äô saved [400395283/400395283]\n",
    "\n",
    "‚úÖ File integrity check passed for spark.tgz\n",
    "‚úÖ Spark installation successful\n",
    "Setting up environment variables...\n",
    "Verifying installations...\n",
    "‚úÖ Kafka found at /root/kafka\n",
    "‚úÖ Hadoop found at /root/hadoop\n",
    "‚úÖ HBase found at /root/hbase\n",
    "‚úÖ Spark found at /root/spark\n",
    "\n",
    "Testing installed versions:\n",
    "Java version:\n",
    "openjdk version \"11.0.25\" 2024-10-15\n",
    "OpenJDK Runtime Environment (build 11.0.25+9-post-Ubuntu-1ubuntu122.04)\n",
    "OpenJDK 64-Bit Server VM (build 11.0.25+9-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n",
    "\n",
    "Kafka version:\n",
    "3.6.0\n",
    "\n",
    "Hadoop version:\n",
    "Hadoop 3.3.6\n",
    "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
    "Compiled by ubuntu on 2023-06-18T08:22Z\n",
    "Compiled on platform linux-x86_64\n",
    "Compiled with protoc 3.7.1\n",
    "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
    "This command was run using /root/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n",
    "\n",
    "HBase version:\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/root/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/root/hbase/lib/client-facing-thirdparty/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
    "HBase 2.5.5\n",
    "Source code repository git://buildbox.localdomain/home/apurtell/tmp/RM/hbase revision=7ebd4381261fefd78fc2acf258a95184f4147cee\n",
    "Compiled by apurtell on Thu Jun  1 17:42:49 PDT 2023\n",
    "From source with checksum cf3b264aa86082e09c1b42609e6af98d84af8804d094bd8b90eb07d4684d40aef2ca9ca0c26dc38206b6011843e56d0618878c2547fd4834ce5744d3417e4f49\n",
    "\n",
    "Spark version:\n",
    "24/12/14 23:18:42 WARN Utils: Your hostname, DESKTOP-2IQJ5FH resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
    "24/12/14 23:18:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.0\n",
    "      /_/\n",
    "                        \n",
    "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.25\n",
    "Branch HEAD\n",
    "Compiled by user ubuntu on 2023-09-09T01:53:20Z\n",
    "Revision ce5ddad990373636e94071e7cef2f31021add07b\n",
    "Url https://github.com/apache/spark\n",
    "Type --help for more information.\n",
    "\n",
    "‚úÖ Installation completed successfully!\n",
    "Please run 'source ~/.bashrc' to apply environment variables\n",
    "root@DESKTOP-2IQJ5FH:~/GIT/Big-Data-Project_2/Main# \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T√¥i hi·ªÉu r·∫±ng b·∫°n mu·ªën chuy·ªÉn t·ª´ c√†i ƒë·∫∑t tr√™n Windows sang Linux s·ª≠ d·ª•ng hai script `install_services.sh` v√† `start_services.sh`. ƒê√¢y l√† c√°c b∆∞·ªõc th·ª±c hi·ªán:\n",
    "\n",
    "1. ƒê·∫ßu ti√™n, c√†i ƒë·∫∑t c√°c services:\n",
    "```bash\n",
    "# C·∫•p quy·ªÅn th·ª±c thi cho script\n",
    "chmod +x install_services.sh\n",
    "\n",
    "# Ch·∫°y script c√†i ƒë·∫∑t\n",
    "./install_services.sh\n",
    "```\n",
    "\n",
    "2. Sau khi c√†i ƒë·∫∑t xong, load c√°c bi·∫øn m√¥i tr∆∞·ªùng:\n",
    "```bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "3. Kh·ªüi ƒë·ªông c√°c services:\n",
    "```bash\n",
    "# C·∫•p quy·ªÅn th·ª±c thi cho script\n",
    "chmod +x start_services.sh\n",
    "\n",
    "# Ch·∫°y script kh·ªüi ƒë·ªông\n",
    "./start_services.sh\n",
    "```\n",
    "\n",
    "So v·ªõi phi√™n b·∫£n Windows, c√≥ m·ªôt s·ªë ƒëi·ªÉm kh√°c bi·ªát ch√≠nh:\n",
    "\n",
    "1. **ƒê∆∞·ªùng d·∫´n**:\n",
    "- Windows: S·ª≠ d·ª•ng `C:/kafka_2.13_2.6.0/...`\n",
    "- Linux: S·ª≠ d·ª•ng `$HOME/kafka/...`\n",
    "\n",
    "2. **L·ªánh kh·ªüi ƒë·ªông**:\n",
    "- Windows: S·ª≠ d·ª•ng `.bat` files\n",
    "- Linux: S·ª≠ d·ª•ng tr·ª±c ti·∫øp binary files\n",
    "\n",
    "3. **T·ª± ƒë·ªông h√≥a**:\n",
    "- Script `install_services.sh` s·∫Ω t·ª± ƒë·ªông:\n",
    "  - C√†i ƒë·∫∑t Java, Python v√† c√°c dependencies\n",
    "  - T·∫£i v√† c√†i ƒë·∫∑t Kafka, Hadoop, HBase, Spark\n",
    "  - C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng\n",
    "\n",
    "- Script `start_services.sh` s·∫Ω t·ª± ƒë·ªông:\n",
    "  - Kh·ªüi ƒë·ªông Zookeeper v√† Kafka\n",
    "  - T·∫°o Kafka topic\n",
    "  - Kh·ªüi ƒë·ªông Hadoop\n",
    "  - Kh·ªüi ƒë·ªông HBase v√† Thrift server\n",
    "  - Kh·ªüi ƒë·ªông Spark\n",
    "  - Kh·ªüi ƒë·ªông Airflow trong Docker\n",
    "\n",
    "4. **Ki·ªÉm tra services**:\n",
    "```bash\n",
    "# Ki·ªÉm tra Kafka topic\n",
    "$KAFKA_HOME/bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "\n",
    "# Ki·ªÉm tra HBase\n",
    "$HBASE_HOME/bin/hbase shell\n",
    "\n",
    "# Ki·ªÉm tra Hadoop\n",
    "hdfs dfs -ls /\n",
    "\n",
    "# Truy c·∫≠p Airflow UI\n",
    "http://localhost:8080\n",
    "```\n",
    "\n",
    "L∆∞u √Ω: C√°c script n√†y ƒë√£ bao g·ªìm c√°c ch·ª©c nƒÉng ki·ªÉm tra l·ªói v√† cleanup, gi√∫p vi·ªác qu·∫£n l√Ω services d·ªÖ d√†ng h∆°n so v·ªõi phi√™n b·∫£n Windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pw: 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C·∫•p sudo cho user hduser\n",
    "```bash\n",
    "sudo usermod -aG sudo hduser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in \n",
    "```bash\n",
    "su - hduser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "root@DESKTOP-2IQJ5FH:~/GIT/Big-Data-Project_2/Main# sudo usermod -aG sudo hduser\n",
    "root@DESKTOP-2IQJ5FH:~/GIT/Big-Data-Project_2/Main# su - hduser\n",
    "To run a command as administrator (user \"root\"), use \"sudo <command>\".\n",
    "See \"man sudo_root\" for details.\n",
    "\n",
    "Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.153.1-microsoft-standard-WSL2 x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management:     https://landscape.canonical.com\n",
    " * Support:        https://ubuntu.com/advantage\n",
    "\n",
    " * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s\n",
    "   just raised the bar for easy, resilient and secure K8s cluster deployment.\n",
    "\n",
    "   https://ubuntu.com/engage/secure-kubernetes-at-the-edge\n",
    "\n",
    "This message is shown once a day. To disable it please create the\n",
    "/home/hduser/.hushlogin file.\n",
    "hduser@DESKTOP-2IQJ5FH:~$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "H√£y ki·ªÉm tra t·ª´ng th√†nh ph·∫ßn:\n",
    "\n",
    "1. **Ki·ªÉm tra Kafka**:\n",
    "```bash\n",
    "# Ki·ªÉm tra topic\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "\n",
    "# Test producer/consumer\n",
    "kafka-console-producer.sh --topic smartphoneTopic --bootstrap-server localhost:9092\n",
    "# (M·ªü terminal kh√°c)\n",
    "kafka-console-consumer.sh --topic smartphoneTopic --from-beginning --bootstrap-server localhost:9092\n",
    "```\n",
    "\n",
    "2. **Ki·ªÉm tra HDFS**:\n",
    "```bash\n",
    "# Ki·ªÉm tra status\n",
    "hdfs dfsadmin -report\n",
    "\n",
    "# Ki·ªÉm tra web UI\n",
    "curl http://localhost:9870\n",
    "```\n",
    "\n",
    "3. **Ki·ªÉm tra HBase**:\n",
    "```bash\n",
    "# V√†o HBase shell\n",
    "hbase shell\n",
    "\n",
    "# Trong HBase shell:\n",
    "status\n",
    "list # ki·ªÉm tra tables\n",
    "```\n",
    "\n",
    "4. **Ki·ªÉm tra Web UIs**:\n",
    "```\n",
    "HDFS: http://localhost:9870\n",
    "YARN: http://localhost:8088\n",
    "HBase: http://localhost:16010\n",
    "```\n",
    "\n",
    "5. **Ki·ªÉm tra logs**:\n",
    "```bash\n",
    "# HDFS logs\n",
    "tail -f $HADOOP_HOME/logs/hadoop-root-namenode-*.log\n",
    "\n",
    "# HBase logs\n",
    "tail -f $HBASE_HOME/logs/hbase-root-master-*.log\n",
    "\n",
    "# Kafka logs\n",
    "tail -f $KAFKA_HOME/logs/server.log\n",
    "```\n",
    "\n",
    "6. **Ki·ªÉm tra processes**:\n",
    "```bash\n",
    "jps\n",
    "```\n",
    "N√™n th·∫•y:\n",
    "- NameNode\n",
    "- DataNode\n",
    "- ResourceManager\n",
    "- NodeManager\n",
    "- HMaster\n",
    "- HRegionServer\n",
    "- QuorumPeerMain (Zookeeper)\n",
    "- Kafka\n",
    "\n",
    "N·∫øu t·∫•t c·∫£ ƒë·ªÅu OK, b·∫°n c√≥ th·ªÉ ti·∫øp t·ª•c ch·∫°y:\n",
    "1. stream_pipeline.py\n",
    "2. Web application (Spring Boot ho·∫∑c Flask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(venv) root@DESKTOP-2IQJ5FH:~/GIT/Big-Data-Project_2# jps\n",
    "49249 DataNode\n",
    "69747 ConsoleProducer\n",
    "52516 HRegionServer\n",
    "52325 HMaster\n",
    "50151 NodeManager\n",
    "42759 ThriftServer\n",
    "47448 Kafka\n",
    "49977 ResourceManager\n",
    "46874 QuorumPeerMain\n",
    "49051 NameNode\n",
    "49502 SecondaryNameNode\n",
    "82254 Jps\n",
    "(venv) root@DESKTOP-2IQJ5FH:~/GIT/Big-Data-Project_2# \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ ki·ªÉm tra tr·∫°ng th√°i c√°c services ƒëang ch·∫°y, b·∫°n c√≥ th·ªÉ d√πng c√°c c√°ch sau:\n",
    "\n",
    "1. Ki·ªÉm tra c√°c process ƒëang ch·∫°y:\n",
    "```bash\n",
    "jps\n",
    "```\n",
    "S·∫Ω hi·ªÉn th·ªã danh s√°ch c√°c Java processes nh∆∞:\n",
    "- NameNode\n",
    "- DataNode\n",
    "- ResourceManager \n",
    "- NodeManager\n",
    "- HMaster\n",
    "- HRegionServer\n",
    "- ThriftServer\n",
    "- QuorumPeerMain (Zookeeper)\n",
    "\n",
    "2. Ki·ªÉm tra c√°c c·ªïng ƒëang m·ªü:\n",
    "```bash\n",
    "netstat -tulpn | grep -E \"2181|9092|9000|8088|16000|16010\"\n",
    "```\n",
    "- 2181: Zookeeper\n",
    "- 9092: Kafka\n",
    "- 9000: HDFS\n",
    "- 8088: YARN ResourceManager\n",
    "- 16000: HBase Master\n",
    "- 16010: HBase Master Web UI\n",
    "\n",
    "3. Truy c·∫≠p Web UI:\n",
    "```\n",
    "HDFS NameNode: http://localhost:9870\n",
    "YARN ResourceManager: http://localhost:8088  \n",
    "HBase Master: http://localhost:16010\n",
    "```\n",
    "\n",
    "4. Ki·ªÉm tra logs:\n",
    "```bash\n",
    "# HDFS logs\n",
    "tail -f $HADOOP_HOME/logs/hadoop-root-namenode-*.log\n",
    "tail -f $HADOOP_HOME/logs/hadoop-root-datanode-*.log\n",
    "\n",
    "# HBase logs  \n",
    "tail -f $HBASE_HOME/logs/hbase-root-master-*.log\n",
    "tail -f $HBASE_HOME/logs/hbase-root-regionserver-*.log\n",
    "\n",
    "# Kafka logs\n",
    "tail -f $KAFKA_HOME/logs/server.log\n",
    "tail -f $KAFKA_HOME/logs/zookeeper.out\n",
    "```\n",
    "\n",
    "5. Ki·ªÉm tra tr·∫°ng th√°i c·ª• th·ªÉ:\n",
    "```bash\n",
    "# HDFS status\n",
    "hdfs dfsadmin -report\n",
    "\n",
    "# YARN status  \n",
    "yarn node -list\n",
    "\n",
    "# HBase status\n",
    "echo \"status\" | hbase shell\n",
    "\n",
    "# Kafka topics\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "```\n",
    "\n",
    "C√°c l·ªánh n√†y s·∫Ω gi√∫p b·∫°n x√°c ƒë·ªãnh tr·∫°ng th√°i ho·∫°t ƒë·ªông c·ªßa t·ª´ng service trong h·ªá th·ªëng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(venv) root@DESKTOP-2IQJ5FH:~/GIT/Big-Data-Project_2# kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "smartphoneTopic\n",
    "(venv) root@DESKTOP-2IQJ5FH:~/GIT/Big-Data-Project_2# \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "T·∫•t c·∫£ c√°c services ƒëang ch·∫°y t·ªët! H√£y ph√¢n t√≠ch:\n",
    "\n",
    "1. **Hadoop Ecosystem**:\n",
    "- `49051 NameNode`: HDFS NameNode ‚úÖ\n",
    "- `49249 DataNode`: HDFS DataNode ‚úÖ\n",
    "- `49502 SecondaryNameNode`: Secondary NameNode ‚úÖ\n",
    "- `49977 ResourceManager`: YARN ResourceManager ‚úÖ\n",
    "- `50151 NodeManager`: YARN NodeManager ‚úÖ\n",
    "\n",
    "2. **HBase Services**:\n",
    "- `52325 HMaster`: HBase Master ‚úÖ\n",
    "- `52516 HRegionServer`: HBase RegionServer ‚úÖ\n",
    "- `42759 ThriftServer`: HBase Thrift Server ‚úÖ\n",
    "\n",
    "3. **Kafka & Zookeeper**:\n",
    "- `47448 Kafka`: Kafka Broker ‚úÖ\n",
    "- `46874 QuorumPeerMain`: Zookeeper ‚úÖ\n",
    "- `69747 ConsoleProducer`: Kafka Producer ƒëang ch·∫°y ‚úÖ\n",
    "\n",
    "4. **Utility**:\n",
    "- `82254 Jps`: Process ƒë·ªÉ li·ªát k√™ Java processes\n",
    "\n",
    "T·∫•t c·∫£ c√°c components ch√≠nh ƒë·ªÅu ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng. B√¢y gi·ªù b·∫°n c√≥ th·ªÉ:\n",
    "\n",
    "1. Ch·∫°y stream pipeline:\n",
    "```bash\n",
    "python stream_pipeline.py\n",
    "```\n",
    "\n",
    "2. Ch·∫°y web application (ch·ªçn 1 trong 2):\n",
    "```bash\n",
    "# Option 1: Spring Boot\n",
    "cd real_time_app\n",
    "./mvnw spring-boot:run\n",
    "\n",
    "# Option 2: Flask\n",
    "cd real_time_web_app\n",
    "python app.py\n",
    "```\n",
    "\n",
    "3. Ki·ªÉm tra d·ªØ li·ªáu ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω:\n",
    "```bash\n",
    "# Ki·ªÉm tra HBase\n",
    "hbase shell\n",
    "> scan 'smartphone_table'\n",
    "\n",
    "# Ki·ªÉm tra Kafka messages\n",
    "kafka-console-consumer.sh --topic smartphoneTopic --from-beginning --bootstrap-server localhost:9092\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
